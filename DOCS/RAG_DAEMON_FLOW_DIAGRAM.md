# RAG Daemon Enrichment Flow - Problem vs Solution

## CURRENT BROKEN FLOW âŒ

```
llmc-rag-service daemon
    â”œâ”€ process_repo()
    â”‚   â””â”€ run_rag_cli(["enrich", "--execute"])
    â”‚       â””â”€ subprocess: python -m tools.rag.cli enrich --execute
    â”‚           â””â”€ cli.py:enrich()
    â”‚               â””â”€ llm = default_enrichment_callable(model)  âš ï¸ FAKE DATA GENERATOR
    â”‚                   â””â”€ workers.py:default_enrichment_callable()
    â”‚                       â””â”€ Returns: {
    â”‚                             "summary_120w": "file.py:10-50 auto-summary generated offline.",  âŒ
    â”‚                             "inputs": [],      âŒ EMPTY
    â”‚                             "outputs": [],     âŒ EMPTY
    â”‚                             "side_effects": [], âŒ EMPTY
    â”‚                             "pitfalls": [],    âŒ EMPTY
    â”‚                           }
    â”‚
    â””â”€ Result: Garbage data pollutes RAG index ğŸ’©
```

**What's Lost:**
- âŒ No real LLM calls (Qwen 7b/14b or GPT-4o-mini)
- âŒ No smart routing (complexity-based tier selection)
- âŒ No GPU monitoring (VRAM, temp, utilization)
- âŒ No retry logic (failures are permanent)
- âŒ No metrics (no visibility into performance)
- âŒ No multi-host support (can't use Athena or failover)

---

## FIXED FLOW âœ…

### Option A: Use runner.refresh (RECOMMENDED)

```
llmc-rag-service daemon
    â”œâ”€ process_repo()
    â”‚   â”œâ”€ Import: from tools.rag.runner import run_enrich, run_sync, run_embed
    â”‚   â”‚
    â”‚   â”œâ”€ Step 1: Detect & sync changes
    â”‚   â”‚   â””â”€ detect_changes(repo) â†’ list of modified files
    â”‚   â”‚   â””â”€ run_sync(repo, changes) â†’ update RAG index
    â”‚   â”‚
    â”‚   â”œâ”€ Step 2: Enrich with REAL LLMs âœ…
    â”‚   â”‚   â””â”€ run_enrich(
    â”‚   â”‚         repo, 
    â”‚   â”‚         backend="ollama",    # or "gateway" or "auto"
    â”‚   â”‚         router="on",         # Enable smart routing
    â”‚   â”‚         start_tier="7b",     # Start fast, escalate if needed
    â”‚   â”‚         batch_size=5,
    â”‚   â”‚         max_spans=50,
    â”‚   â”‚         cooldown=0
    â”‚   â”‚       )
    â”‚   â”‚       â””â”€ Calls: scripts/qwen_enrich_batch.py
    â”‚   â”‚           â””â”€ Sophisticated 1,407-line routing system
    â”‚   â”‚               â”œâ”€ Analyze code complexity
    â”‚   â”‚               â”‚   â”œâ”€ Line count
    â”‚   â”‚               â”‚   â”œâ”€ Nesting depth
    â”‚   â”‚               â”‚   â”œâ”€ Schema complexity
    â”‚   â”‚               â”‚   â””â”€ Token estimation
    â”‚   â”‚               â”‚
    â”‚   â”‚               â”œâ”€ Choose starting tier
    â”‚   â”‚               â”‚   â”œâ”€ Simple code â†’ Qwen 7b (fast & cheap)
    â”‚   â”‚               â”‚   â”œâ”€ Complex code â†’ Qwen 14b (smarter)
    â”‚   â”‚               â”‚   â””â”€ Edge cases â†’ GPT-4o-mini (fallback)
    â”‚   â”‚               â”‚
    â”‚   â”‚               â”œâ”€ Call real LLM with prompt:
    â”‚   â”‚               â”‚   {
    â”‚   â”‚               â”‚     "path": "file.py",
    â”‚   â”‚               â”‚     "lines": [10, 50],
    â”‚   â”‚               â”‚     "code": "actual code here...",
    â”‚   â”‚               â”‚     "task": "Analyze and summarize this code..."
    â”‚   â”‚               â”‚   }
    â”‚   â”‚               â”‚
    â”‚   â”‚               â”œâ”€ Monitor GPU during processing
    â”‚   â”‚               â”‚   â”œâ”€ VRAM usage
    â”‚   â”‚               â”‚   â”œâ”€ Temperature
    â”‚   â”‚               â”‚   â”œâ”€ Utilization %
    â”‚   â”‚               â”‚   â””â”€ Power draw
    â”‚   â”‚               â”‚
    â”‚   â”‚               â”œâ”€ Validate LLM response
    â”‚   â”‚               â”‚   â””â”€ Check against enrichment schema
    â”‚   â”‚               â”‚
    â”‚   â”‚               â”œâ”€ On failure:
    â”‚   â”‚               â”‚   â”œâ”€ Classify error type
    â”‚   â”‚               â”‚   â”œâ”€ Escalate to next tier (7bâ†’14bâ†’nano)
    â”‚   â”‚               â”‚   â”œâ”€ Or failover to next host
    â”‚   â”‚               â”‚   â””â”€ Retry with backoff
    â”‚   â”‚               â”‚
    â”‚   â”‚               â””â”€ Log comprehensive metrics:
    â”‚   â”‚                   â”œâ”€ Token counts (in/out)
    â”‚   â”‚                   â”œâ”€ Duration per span
    â”‚   â”‚                   â”œâ”€ GPU stats
    â”‚   â”‚                   â”œâ”€ Tier usage distribution
    â”‚   â”‚                   â”œâ”€ Success/failure rates
    â”‚   â”‚                   â””â”€ Cost estimation
    â”‚   â”‚
    â”‚   â””â”€ Step 3: Generate embeddings
    â”‚       â””â”€ run_embed(repo, limit=100)
    â”‚
    â””â”€ Result: High-quality enrichment data âœ¨
```

**What's Gained:**
- âœ… Real intelligent summaries from actual LLMs
- âœ… Smart routing saves $$$ (7b for simple, 14b for complex)
- âœ… GPU monitoring prevents OOM and tracks utilization
- âœ… Robust retry logic handles transient failures
- âœ… Comprehensive metrics for optimization
- âœ… Multi-host support (Athena failover, load balancing)
- âœ… Token tracking for cost analysis
- âœ… Automatic tier escalation on validation failures

---

## Code Change Required

### Before (BROKEN):
```python
# tools/rag/service.py line 226
def process_repo(self, repo_path: str):
    repo = Path(repo_path)
    print(f"ğŸ”„ Processing {repo.name}...")
    
    # âŒ Uses CLI which calls fake enrichment
    success, output = self.run_rag_cli(repo, ["enrich", "--execute"])
    if not success:
        print(f"  âš ï¸  Enrichment had failures")
```

### After (FIXED):
```python
# tools/rag/service.py line 226
def process_repo(self, repo_path: str):
    repo = Path(repo_path)
    print(f"ğŸ”„ Processing {repo.name}...")
    
    # âœ… Import proper runner functions
    from tools.rag.runner import run_enrich, run_sync, run_embed, detect_changes
    from tools.rag.config import index_path_for_write
    
    # Sync changes
    index_path = index_path_for_write(repo)
    changes = detect_changes(repo, index_path=index_path)
    if changes:
        run_sync(repo, changes)
    
    # âœ… Call real enrichment with routing
    run_enrich(
        repo,
        backend=os.getenv("ENRICH_BACKEND", "ollama"),
        router=os.getenv("ENRICH_ROUTER", "on"),
        start_tier=os.getenv("ENRICH_START_TIER", "7b"),
        batch_size=5,
        max_spans=50,
        cooldown=0
    )
    
    # Generate embeddings
    run_embed(repo, limit=100)
```

---

## Example Real Output

### BEFORE (FAKE):
```json
{
  "summary_120w": "src/utils/parser.py:45-89 auto-summary generated offline.",
  "inputs": [],
  "outputs": [],
  "side_effects": [],
  "pitfalls": [],
  "usage_snippet": "def parse_config(...):\n    ...",
  "tags": []
}
```

### AFTER (REAL):
```json
{
  "summary_120w": "Parses YAML configuration files with schema validation. Loads config from disk, validates against predefined schema using jsonschema, applies default values for missing fields, and returns validated config dict. Raises ConfigError on validation failures. Caches parsed configs in memory using functools.lru_cache for performance.",
  "inputs": ["config_path: Path", "schema: dict", "use_cache: bool = True"],
  "outputs": ["Dict[str, Any]: Validated configuration dictionary"],
  "side_effects": ["File I/O: reads from disk", "Cache: stores in LRU cache"],
  "pitfalls": ["ConfigError on missing required fields", "YAML parsing errors on malformed files", "Cache can become stale if file changes"],
  "usage_snippet": "config = parse_config(Path('config.yaml'), SCHEMA)",
  "tags": ["config", "yaml", "validation", "caching"],
  "model": "qwen2.5:7b-instruct-q4_K_M",
  "tier": "7b",
  "tokens_in": 342,
  "tokens_out": 156,
  "duration_ms": 1234,
  "gpu_vram_peak_mib": 4523
}
```

---

## Performance Impact

### Token Savings Example (100 spans):
```
BROKEN approach:
- 100 spans Ã— 0 tokens = 0 tokens
- Cost: $0.00 (but also $0.00 value)
- Quality: 0% useful

FIXED approach with routing:
- 70 simple spans Ã— 350 tokens Ã— $0.0001/1K = $0.24 (7b local, essentially free)
- 25 medium spans Ã— 800 tokens Ã— $0.0002/1K = $0.40 (14b local, essentially free)  
- 5 complex spans Ã— 1500 tokens Ã— $0.15/1M = $0.001 (GPT-4o-mini API)
- Total cost: ~$0.65 for 100 spans
- Quality: 95%+ useful
- Time saved searching: hours
```

### GPU Utilization:
```
BROKEN: 0% GPU usage (no LLM calls)
FIXED:  40-60% GPU usage during processing (actual work)
        With monitoring to prevent OOM crashes
```

---

## Priority: P0 - IMMEDIATE FIX REQUIRED

This is not a minor bug - it's a **complete system failure** that makes the daemon worthless for its primary purpose.

**Fix this before doing ANY other daemon work.**
