# LLMC Local Configuration Example
# Copy this file to 'local.toml' and modify as needed
# This file is git-ignored by default

# Override default embeddings configuration
[embeddings]
# Uncomment and modify to use a different model
# preset = "e5-large"
# model = "your-custom-embedding-model"

# Override storage paths
[storage]
# Use custom paths for index and cache
# index_path = "/custom/path/index.db"
# cache_path = "/custom/cache/directory"

# Enable enrichment with custom settings
[enrichment]
# Uncomment to enable document enrichment
# enabled = true
# model = "local-llm-or-api-model"
# batch_size = 25

# Customize logging
[logging]
# Set your preferred log level
# level = "DEBUG"
# Disable console output for production
# console_output = false
# Custom log file path
# file = "/var/log/llmc.log"

# Enable concurrency for parallel processing
[concurrency]
# Uncomment to enable parallel task processing
# enabled = true
# max_concurrent_tasks = 8
# task_timeout = 600

# Configure semantic caching
[semantic_cache]
# Adjust cache sensitivity
# min_score = 0.8
# Disable semantic cache if needed
# enabled = false

# Enable deep research recommendations
[deep_research]
# Uncomment to enable deep research detection
# enabled = true
# Allow automatic route overrides
# auto_route_override = true

# Configure model providers
[providers]
# Set your preferred default provider
# default = "ollama"  # or "gemini", "minimax", etc.

# Claude provider settings
[providers.claude]
# Uncomment and set your API key environment variable name
# api_key_env = "CLAUDE_API_KEY"

# Azure OpenAI configuration
[providers.azure]
# Uncomment and configure for Azure OpenAI
# enabled = true
# endpoint_env = "AZURE_OPENAI_ENDPOINT"
# key_env = "AZURE_OPENAI_KEY"
# deployment_env = "AZURE_OPENAI_DEPLOYMENT"

# Gemini configuration
[providers.gemini]
# Uncomment and set for Gemini API
# enabled = true
# api_key_env = "GOOGLE_AI_API_KEY"

# Ollama local configuration
[providers.ollama]
# Customize Ollama settings
# base_url = "http://localhost:11434"
# model = "llama3.1:8b"  # or your preferred local model

# MiniMax API configuration
[providers.minimax]
# Uncomment and configure for MiniMax API
# enabled = true
# api_key_env = "MINIMAX_API_KEY"
# model_env = "MINIMAX_MODEL"

# Customize RAG behavior
[rag]
# Adjust search parameters
# min_score = 0.5
# min_confidence = 0.7
# max_results = 15
# Disable RAG if not needed
# enabled = false

# Security settings
[security]
# Enable dangerous mode (use with caution!)
# dangerously_skip_permissions = true
# yolo_mode = true
# Disable confirmations for automation
# require_confirmation = false

# Template configuration
[templates]
# Don't copy templates on first use
# copy_on_first_use = false
# Don't backup existing templates
# backup_existing = false

# Performance tuning
[performance]
# Increase cache TTL for better reuse
# cache_ttl = 7200
# Allow more memory usage
# max_memory_usage = "2GB"
# More frequent garbage collection
# gc_frequency = 50

# Development settings
[development]
# Enable debug mode
# debug = true
# Enable trace logging
# trace_logging = true
# Profile database queries
# profile_queries = true

# UI customization
[ui]
# Disable colored output
# color_output = false
# Hide progress bars
# progress_bars = false
# Show more verbose errors
# verbose_errors = true
# Disable interactive mode
# interactive_mode = false