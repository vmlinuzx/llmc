# Ollama Provider Configuration
# Local LLM inference with Ollama

name: ollama
display_name: "Ollama (Local)"
description: "Local LLM inference using Ollama - free and private"

# Provider settings
provider:
  type: "ollama"
  enabled: true
  base_url: "http://localhost:11434"
  api_key: null  # No API key needed for local

# Model configuration
model:
  name: "qwen2.5:14b"  # Default model
  parameters:
    temperature: 0.1
    top_p: 0.9
    num_predict: 4096
    num_ctx: 8192

# Supported models
supported_models:
  - "llama3.1:8b"
  - "llama3.1:70b"
  - "qwen2.5:7b"
  - "qwen2.5:14b"
  - "qwen2.5:32b"
  - "codellama:7b"
  - "codellama:13b"
  - "mistral:7b"
  - "phi3:mini"
  - "phi3:medium"
  - "gemma2:2b"
  - "gemma2:9b"
  - "gemma2:27b"

# Context and memory
context:
  max_context_length: 8192  # Varies by model
  summary_threshold: 0.7
  compression_ratio: 0.2

# Features
features:
  tools: false  # Limited tool support
  function_calling: false
  streaming: true
  vision: true  # For vision models
  code_generation: true
  multimodal: true  # For multimodal models

# Performance settings
performance:
  timeout: 600  # Longer timeout for local processing
  retry_attempts: 1  # Don't retry local failures
  rate_limit:
    requests_per_minute: 60
    tokens_per_minute: 10000  # Depends on hardware

# Cost optimization
cost:
  tier: "free"
  budget_alerts: false
  usage_tracking: false
  cost_per_token: 0.0  # Completely free after setup

# Hardware requirements
hardware:
  cpu: "Modern x86_64 or ARM64"
  ram: "8GB minimum, 16GB+ recommended"
  gpu: "Optional, improves speed significantly"
  storage: "10GB+ for models"

# Setup instructions
setup:
  - "Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh"
  - "Pull a model: ollama pull qwen2.5:14b"
  - "Start Ollama: ollama serve"
  - "Test: curl http://localhost:11434/api/tags"

# Integration settings
integration:
  health_check: "http://localhost:11434/api/tags"
  environment_variables: []
  headers: {}

# Use cases
use_cases:
  - "privacy_sensitive_tasks"
  - "offline_development"
  - "cost_free_processing"
  - "experimentation"
  - "local_prototyping"
  - "educational_purposes"

# Limitations
limitations:
  - "slower_than_cloud_apis"
  - "limited_model_choices"
  - "no_tool_calling"
  - "resource_intensive"
  - "model_dependent_quality"

# Quality tiers by model
model_tiers:
  fast:
    - "phi3:mini"
    - "gemma2:2b"
    - "qwen2.5:7b"
  
  balanced:
    - "llama3.1:8b"
    - "qwen2.5:14b"
    - "gemma2:9b"
  
  high_quality:
    - "llama3.1:70b"
    - "qwen2.5:32b"
    - "gemma2:27b"
  
  code_focused:
    - "codellama:7b"
    - "codellama:13b"