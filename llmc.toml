# ==============================================================================
# Embeddings
# ==============================================================================
# 
# 
# 

[embeddings]
default_profile = "docs"

[embeddings.profiles.docs]
provider = "ollama"
model = "nomic-embed-text"
dimension = 768

[embeddings.profiles.docs.ollama]
api_base = "http://192.168.5.20:11434"
timeout = 60

[embeddings.profiles.code]
provider = "ollama"
model = "nomic-embed-text"
dimension = 768

[embeddings.profiles.code.ollama]
api_base = "http://192.168.5.20:11434"
timeout = 60

[embeddings.profiles.cheap]
provider = "ollama"
model = "all-minilm"
dimension = 384

[embeddings.profiles.cheap.ollama]
api_base = "http://192.168.5.20:11434"
timeout = 60

[embeddings.profiles.test]
provider = "hash"
dimension = 64

[embeddings.profiles.code_jina]
provider = "ollama"
model = "jina-embeddings-v2-base-code-Q5_K_M.gguf"
dim = 768
cost_class = "low"
capabilities = ["code"]

[embeddings.routes.code]
profile = "code_jina"
index = "emb_code"

[embeddings.routes.docs]
profile = "docs"
index = "embeddings"

[embeddings.routes.erp]
profile = "docs"
index = "emb_erp"

[routing.slice_type_to_route]
code = "code"
docs = "docs"
erp_product = "erp"
config = "docs"
data = "docs"
other = "docs"

[routing.options]
enable_query_routing = true
enable_multi_route = false

# Phase 6: Multi-route retrieval configuration
# Define a primary route and one or more secondary routes to fan out to.
[routing.multi_route.code_primary]
primary = "code"
secondary = [
  { route = "docs", weight = 0.5 }
]

[routing.multi_route.docs_primary]
primary = "docs"
secondary = [
  { route = "code", weight = 0.3 }
]

gpu_min_free_mb = 1536

[storage]
# index_path = ".rag/index_v2.db"

[daemon]
# Automatic cleanup of Python bytecode cache (.llmc/pycache/)
# Prevents unbounded file accumulation from long-running daemon processes
pycache_cleanup_days = 7  # Delete .pyc files older than N days (0 = disabled)

# Idle Loop Throttling - reduce CPU usage when no work to do
nice_level = 10                    # Process priority (0-19, higher = lower priority)
idle_backoff_max = 10              # Max multiplier when idle (10x = 30min at default interval)
idle_backoff_base = 2              # Exponential base (2^n)



# ==============================================================================
# Enrichment
# ==============================================================================
# 
# 
# 


[enrichment]
#enabled = true
default_chain = "athena" #Athena is the name of my local LLM.  You MUST have a default_chain defined and at least one llm URL for this to work!
batch_size = 50 #Number of spans to enrich before doing full enrichment loop.  50 is pretty good on a strix halo using 7b/14b.  If you have slower inference considering lowering this.
est_tokens_per_span = 350
enforce_latin1_enrichment = true
max_failures_per_span = 4 #Stop retrying after this many failures.  Especially important if you are using a paid provider.

[[enrichment.chain]]
name = "athena"
chain = "athena"
provider = "ollama"
model = "qwen2.5:7b-instruct"
url = "http://192.168.5.20:11434"
routing_tier = "7b"
timeout_seconds = 120
enabled = true
options = { num_ctx = 8192, temperature = 0.2 }

[[enrichment.chain]]
name = "athena-14b"
chain = "athena"
provider = "ollama"
model = "qwen2.5:14b-instruct-q4_K_M"
url = "http://192.168.5.20:11434"
routing_tier = "14b"
timeout_seconds = 180
enabled = true
options = { num_ctx = 12288, temperature = 0.2 }

# [[enrichment.chain]]
# name = "groq-70b"
# chain = "groqathena"
# provider = "gateway"
# model = "groq/llama3-70b-8192"
# routing_tier = "14b"
# enabled = true

# [[enrichment.chain]]
# name = "google-flash"
# chain = "google"
# provider = "gemini"
# model = "gemini-1.5-flash"
# routing_tier = "nano"
# enabled = true


# ==============================================================================
# INDEXING
# ==============================================================================
# 
# 
# 


[indexing]
exclude_dirs = [
    ".git",
    ".rag",
    "node_modules",
    "dist",
    "build",
    "__pycache__",
    ".venv",
    "venv",
    ".next",
    ".cache",
    ".pytest_cache",
    "coverage",
    ".DS_Store",
    "Thumbs.db",
    "tools/rag/quality_check",
]


# ==============================================================================
# LOGGING
# ==============================================================================
# 
# 
# 


[logging]
max_file_size_mb = 10
keep_jsonl_lines = 1000
enable_rotation = true
log_directory = "logs"
auto_rotation_interval = 0


# ==============================================================================
# Tool Envelope (TE) - Shell middleware for intelligent output handling
# ==============================================================================
# TE intercepts standard commands and returns enriched, ranked,
# progressively-disclosed output. The response stream IS the teaching.
# Philosophy: LLMs are alien intelligences that already know Unix.

[tool_envelope]
enabled = true

[tool_envelope.workspace]
root = "/home/vmlinux/src/llmc"
respect_gitignore = true
allow_outside_root = false

[tool_envelope.telemetry]
enabled = true
path = ".llmc/te_telemetry.db"
capture_output = true  # Store actual command output for review (privacy/storage tradeoff)
output_max_bytes = 8192  # Max bytes to capture per command

[tool_envelope.grep]
enabled = true  # Disable enrichment, passthrough with logging only
preview_matches = 10
max_output_chars = 20000
timeout_ms = 5000

[tool_envelope.cat]
enabled = true  # Disable enrichment, passthrough with logging only
preview_lines = 50
max_output_chars = 30000

[tool_envelope.agent_budgets]
gemini-shell = 900000
claude-dc = 180000
qwen-local = 28000
human-cli = 50000
default = 16000


# ==============================================================================
# MCP CONFIGURATION - Model Context Protocol Server
# ==============================================================================
# stdio transport for Claude Desktop; HTTP deferred to later milestone.
# Config precedence: CLI → ENV → TOML → defaults

[mcp]
enabled = true
config_version = "v0"

# ==============================================================================
# CODE EXECUTION MODE - Anthropic "Code Mode" pattern (Phase 2)
# ==============================================================================
# When enabled, replaces 23 tools with 3 bootstrap tools + code execution.
# Claude navigates .llmc/stubs/ filesystem, reads tool definitions on-demand,
# writes Python code that imports and calls them. 98% token reduction.
# Reference: https://www.anthropic.com/engineering/code-execution-with-mcp

[mcp.code_execution]
enabled = true  # Flip to true when ready for code execution mode
stubs_dir = ".llmc/stubs"  # Generated tool stubs for Claude to browse
sandbox = "subprocess"  # "subprocess" | "docker" | "nsjail" (future)
timeout = 30
max_output_bytes = 65536
# Bootstrap tools exposed when code_execution.enabled = true
# All other tools become importable stubs, not MCP-registered tools
bootstrap_tools = ["list_dir", "read_file", "execute_code"]

[mcp.server]
# stdio transport - no host/port needed for Claude Desktop
transport = "stdio"
log_level = "info"

[mcp.auth]
# stdio doesn't need auth (process isolation)
mode = "none"

[mcp.tools]
# Filesystem access roots - constrained to repo for MVP
allowed_roots = ["/home/vmlinux/src/llmc"]
enable_run_cmd = true  # M3: Command execution enabled
run_cmd_allowlist = [
    "bash", "sh", "rg", "grep", "cat", "head", "tail",
    "ls", "find", "wc", "sort", "uniq",
    "python", "python3", "pip",
    "git"
]
# Custom executable tools (exposed as named tools)
# executables = { my_script = "scripts/my_script.sh", deploy = "scripts/deploy.sh" }

read_timeout = 10
exec_timeout = 30

[mcp.rag]
# RAG adapter settings
jit_context_enabled = true
default_scope = "repo"
top_k = 3
token_budget = 600

[mcp.limits]
max_request_bytes = 262144
max_response_bytes = 1048576

[mcp.observability]
# M4: Observability - metrics, logging, token audit
enabled = true
log_format = "json"  # "json" | "text"
log_level = "info"
include_correlation_id = true
# Metrics (in-memory for stdio; get_metrics tool to inspect)
metrics_enabled = true
# Token audit CSV trail
csv_token_audit_enabled = true
csv_path = "./artifacts/mcp_token_audit.csv"
retention_days = 0  # 0 = keep forever

# ==============================================================================
# RAG
# ==============================================================================
# RAG (Retrieval-Augmented Generation) settings

# ==============================================================================
# RAG
# ==============================================================================
# RAG (Retrieval-Augmented Generation) settings

[rag]
enabled = true

# ==============================================================================
# Profiles
# ==============================================================================
# Configuration profiles for different modes

[profiles.daily]
provider = "anthropic"
model = "claude-3-5-sonnet-latest"
temperature = 0.3

[profiles.daily.rag]
enabled = true

[profiles.yolo]
provider = "minimax"
model = "m2-lite"
temperature = 0.2

[profiles.yolo.rag]
enabled = true
