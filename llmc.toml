[embeddings]
default_profile = "docs"

[embeddings.profiles.docs]
provider = "ollama"
model = "nomic-embed-text"
dimension = 768

[embeddings.profiles.docs.ollama]
api_base = "http://192.168.5.20:11434"
timeout = 60

[embeddings.profiles.code]
provider = "ollama"
model = "nomic-embed-text"
dimension = 768

[embeddings.profiles.code.ollama]
api_base = "http://192.168.5.20:11434"
timeout = 60

[embeddings.profiles.cheap]
provider = "ollama"
model = "all-minilm"
dimension = 384

[embeddings.profiles.cheap.ollama]
api_base = "http://192.168.5.20:11434"
timeout = 60

[embeddings.profiles.test]
provider = "hash"
dimension = 64

gpu_min_free_mb = 1536

[storage]
# index_path = ".rag/index_v2.db"

[daemon]
# Automatic cleanup of Python bytecode cache (.llmc/pycache/)
# Prevents unbounded file accumulation from long-running daemon processes
pycache_cleanup_days = 7  # Delete .pyc files older than N days (0 = disabled)


[enrichment]
#enabled = true
default_chain = "athena" #Athena is the name of my local LLM.  You MUST have a default_chain defined and at least one llm URL for this to work!
batch_size = 50 #Number of spans to enrich before doing full enrichment loop.  50 is pretty good on a strix halo using 7b/14b.  If you have slower inference considering lowering this.
est_tokens_per_span = 350
enforce_latin1_enrichment = true
max_failures_per_span = 4 #Stop retrying after this many failures.  Especially important if you are using a paid provider.

[[enrichment.chain]]
name = "athena"
chain = "athena"
provider = "ollama"
model = "qwen2.5:7b-instruct"
url = "http://192.168.5.20:11434"
routing_tier = "7b"
timeout_seconds = 120
enabled = true
options = { num_ctx = 8192, temperature = 0.2 }

[[enrichment.chain]]
name = "athena-14b"
chain = "athena"
provider = "ollama"
model = "qwen2.5:14b-instruct-q4_K_M"
url = "http://192.168.5.20:11434"
routing_tier = "14b"
timeout_seconds = 180
enabled = true
options = { num_ctx = 12288, temperature = 0.2 }

# [[enrichment.chain]]
# name = "groq-70b"
# chain = "groqathena"
# provider = "gateway"
# model = "groq/llama3-70b-8192"
# routing_tier = "14b"
# enabled = false

# [[enrichment.chain]]
# name = "google-flash"
# chain = "google"
# provider = "gemini"
# model = "gemini-1.5-flash"
# routing_tier = "nano"
# enabled = false

[indexing]
exclude_dirs = [
    ".git",
    ".rag",
    "node_modules",
    "dist",
    "build",
    "__pycache__",
    ".venv",
    "venv",
    ".next",
    ".cache",
    ".pytest_cache",
    "coverage",
    ".DS_Store",
    "Thumbs.db",
]





[logging]
max_file_size_mb = 10
keep_jsonl_lines = 1000
enable_rotation = true
log_directory = "logs"
auto_rotation_interval = 0


# ==============================================================================
# Tool Envelope (TE) - Shell middleware for intelligent output handling
# ==============================================================================
# TE intercepts standard commands and returns enriched, ranked,
# progressively-disclosed output. The response stream IS the teaching.
# Philosophy: LLMs are alien intelligences that already know Unix.

[tool_envelope]
enabled = true

[tool_envelope.workspace]
root = "/home/vmlinux/src/llmc"
respect_gitignore = true
allow_outside_root = false

[tool_envelope.telemetry]
enabled = true
path = ".llmc/te_telemetry.jsonl"
capture_output = true  # Store actual command output for review (privacy/storage tradeoff)
output_max_bytes = 8192  # Max bytes to capture per command

[tool_envelope.grep]
enabled = false  # Disable enrichment, passthrough with logging only
preview_matches = 10
max_output_chars = 20000
timeout_ms = 5000

[tool_envelope.cat]
enabled = false  # Disable enrichment, passthrough with logging only
preview_lines = 50
max_output_chars = 30000

[tool_envelope.agent_budgets]
gemini-shell = 900000
claude-dc = 180000
qwen-local = 28000
human-cli = 50000
default = 16000
