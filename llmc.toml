# ==============================================================================
# LLMC Main Configuration
# ==============================================================================
# This is the central configuration file for the LLMC application.
# It contains settings for all major components, including:
# - [agent]: The default agent configuration.
# - [embeddings]: Embedding models and profiles.
# - [routing]: Query routing to different embedding models.
# - [daemon]: The background file processing daemon.
# - [enrichment]: Data enrichment chains and models.
# - [mcp]: The Model Context Protocol server.
# - [tool_envelope]: The shell command middleware.
# - [rag]: Retrieval-Augmented Generation settings.
# - [profiles]: Different configuration profiles for various use cases.
#
# For more details on the configuration options, see the documentation.
#

# ==============================================================================
# Agent (bx) Configuration
# ==============================================================================

[agent]
# provider = "openai"  # Use llama-server for GPT-OSS-120B
model = "qwen3-next-80b-tools"  # Boxxie: 80B MoE @ 32 t/s - uses native tool calling template

[ollama]
url = "http://athena:11434"
timeout = 300

# ==============================================================================
# OpenAI-Compatible Backend (llama.cpp server)
# ==============================================================================
# Use with: bx --model gpt-oss-120b (after setting provider = "openai" in [agent])
# Or use the gpt-oss profile: bx --model gpt-oss-120b-F16.gguf
# 
# llama-server runs on http://athena:8080 with Vulkan GPU acceleration.
# Service: systemctl --user status llama-server
# Supports tool calling via OpenAI-compatible API.

[openai]
url = "http://athena:8080/v1"
timeout = 300
temperature = 0.6
model = "gpt-oss-120b-F16.gguf"  # Model name as llama-server knows it


# ==============================================================================
# Embeddings
# ==============================================================================
# 
# 
# 

[embeddings]
default_profile = "docs"

[embeddings.profiles.docs]
provider = "ollama"
model = "nomic-embed-text"
dimension = 768

[embeddings.profiles.docs.ollama]
api_base = "http://localhost:11434"
timeout = 120

[embeddings.profiles.code]
provider = "ollama"
model = "nomic-embed-text"
dimension = 768

[embeddings.profiles.code.ollama]
api_base = "http://localhost:11434"
timeout = 60

[embeddings.profiles.cheap]
provider = "ollama"
model = "all-minilm"
dimension = 384

[embeddings.profiles.cheap.ollama]
api_base = "http://localhost:11434"
timeout = 60

[embeddings.profiles.test]
provider = "hash"
dimension = 64

[embeddings.profiles.code_jina]
provider = "ollama"
model = "nomic-embed-text"
dimension = 768
cost_class = "low"
capabilities = ["code"]

[embeddings.profiles.code_jina.ollama]
api_base = "http://localhost:11434"
timeout = 120

[embeddings.routes.code]
profile = "code_jina"
index = "emb_code"

[embeddings.routes.docs]
profile = "docs"
index = "embeddings"

[embeddings.routes.erp]
profile = "docs"
index = "embeddings"  # Fixed: was emb_erp (doesn't exist)

[routing.slice_type_to_route]
code = "code"
docs = "docs"
erp_product = "erp"
config = "docs"
data = "docs"
other = "docs"

[routing.options]
enable_query_routing = true
enable_multi_route = true  # P0 FIX: Enable so docs queries also search code index

# Phase 6: Multi-route retrieval configuration
# Define a primary route and one or more secondary routes to fan out to.
[routing.multi_route.code_primary]
primary = "code"
secondary = [
  { route = "docs", weight = 0.5 }
]

[routing.multi_route.docs_primary]
primary = "docs"
secondary = [
  { route = "code", weight = 1.0 }  # Equal weight - let scoring decide winner
] 

gpu_min_free_mb = 1536

[storage]
# index_path = ".rag/index_v2.db"

[daemon]
# Automatic cleanup of Python bytecode cache (.llmc/pycache/)
# Prevents unbounded file accumulation from long-running daemon processes
pycache_cleanup_days = 7  # Delete .pyc files older than N days (0 = disabled)

# Service Mode: "event" (inotify, ~0% CPU when idle) or "poll" (legacy)
mode = "event"

# Event-Driven Mode Settings (only applies when mode = "event")
debounce_seconds = 2.0             # Wait after last file change before processing
housekeeping_interval = 300        # Periodic maintenance (vacuum, log rotation) in seconds

# Poll Mode Settings (only applies when mode = "poll")
# Idle Loop Throttling - reduce CPU usage when no work to do
nice_level = 19                    # Process priority (0-19, higher = lower priority) - MAXED OUT
idle_backoff_max = 2               # Max multiplier when idle (2x = 60s with 30s interval)
idle_backoff_base = 1.5            # Gradual ramp: 30s → 45s → 60s cap

# ==============================================================================
# IDLE ENRICHMENT - Run enrichment when daemon has nothing else to do
# ==============================================================================
# When the daemon is idle (no file changes to process), it can opportunistically
# run enrichment batches to fill in missing summaries using configured chains.

[daemon.idle_enrichment]
enabled = true                     # Enable/disable idle enrichment
batch_size = 10                    # Spans to enrich per idle cycle
interval_seconds = 600             # Minimum seconds between enrichment runs
preferred_chain = "code_enrichment_models"  # Which chain to use (matches [enrichment.routes])
code_first = true                  # Prioritize code spans over docs
max_daily_cost_usd = 1.00          # Stop enrichment if daily cost exceeds this (cloud backends)
dry_run = false                    # If true, log what would be enriched but don't call LLM


# ==============================================================================
# DOCUMENTATION GENERATION (Docgen v2)
# ==============================================================================
# Deterministic, RAG-aware documentation generation for repository files.
# Generates per-file docs in DOCS/REPODOCS/ with SHA256-based idempotence.

[docs.docgen]
enabled = false  # Set to true to enable docgen
backend = "shell"  # "shell" | "llm" | "http" | "mcp"
output_dir = "DOCS/REPODOCS"  # Relative to repo root
require_rag_fresh = true  # Only generate docs for RAG-indexed files

# Shell backend configuration
[docs.docgen.shell]
script = "scripts/docgen_stub.py"
timeout_seconds = 60

# Daemon integration (Phase 9 - not yet implemented)
# daemon_interval_seconds = 3600  # Run every hour
# daemon_batch_size = 10  # Process N files per tick


# ==============================================================================
# Enrichment
# ==============================================================================
# 
# 
# 

[enrichment]
#enabled = true
default_chain = "code_enrichment_models" # The default chain for enrichment when routing doesn't match. Must match a [[enrichment.chain]] entry's "chain" value.
batch_size = 50 #Number of spans to enrich before doing full enrichment loop.  50 is pretty good on a strix halo using 7b/14b.  If you have slower inference considering lowering this.
est_tokens_per_span = 350
enforce_latin1_enrichment = true
max_failures_per_span = 4 #Stop retrying after this many failures.  Especially important if you are using a paid provider.
enable_routing = true  # Enable content-type based routing

# =============================================================================
# OLLAMA OPTIONS REFERENCE
# =============================================================================
# These options can be set in [enrichment.chain.options] for any Ollama backend.
#
# MEMORY/PERFORMANCE:
#   keep_alive = "0"      # Unload model immediately after use (default, saves memory)
#   keep_alive = "5m"     # Keep model loaded for 5 minutes (faster if reused)
#   keep_alive = "24h"    # Keep model loaded for 24 hours (use with caution!)
#   num_ctx = 8192        # Context window size (higher = more memory, longer context)
#   num_batch = 512       # Batch size for prompt processing
#   num_gpu = 99          # Number of layers to offload to GPU (99 = all)
#
# GENERATION:
#   temperature = 0.2     # Creativity (0.0 = deterministic, 1.0 = creative)
#   top_p = 0.9           # Nucleus sampling threshold
#   top_k = 40            # Top-k sampling
#   repeat_penalty = 1.1  # Penalty for repeating tokens
#
# NETWORK (not in options, set at chain level):
#   timeout_seconds = 120 # Request timeout (increase for slow inference)
#   url = "http://localhost:11434"  # Ollama server URL
#   connect_timeout = 10  # Connection timeout in seconds (set in options)
#

# ==============================================================================
# Content-Type Routing - Route different content types to different chains
# ==============================================================================
# slice_type values: code, docs, erp_product, config, data, other
[enrichment.routes]
docs = "document_routes"  # 
code = "code_enrichment_models"        # Route code to your local models


# Fallback to 8B for better quality if 3B fails
#[[enrichment.chain]]
#name = "athena-8b"
#chain = "code_enrichment_models"
#provider = "ollama"
#model = "hf.co/bartowski/Ministral-8B-Instruct-2410-GGUF:F16"
#url = "http://localhost:11434"
#routing_tier = "8b"
#timeout_seconds = 120
#enabled = true
#options = { num_ctx = 8192, temperature = 0.2 }


# OLD: Qwen2.5 7b - replaced by Qwen3 4b (better quality, similar speed)
# [[enrichment.chain]]
# name = "qwen2.5 7b instruct"
# chain = "code_enrichment_models"
# provider = "ollama"
# model = "qwen2.5:7b-instruct"
# url = "http://localhost:11434"
# routing_tier = "7b"
# timeout_seconds = 120
# enabled = true
# options = { num_ctx = 8192, temperature = 0.2 }

# DESKTOP-NBHNN60 (Tailscale) - PRIMARY enrichment server (offloads from laptop!)
[[enrichment.chain]]
name = "desktop qwen3 4b"
chain = "code_enrichment_models"
provider = "ollama"
model = "qwen3:4b-instruct"
url = "http://100.64.0.6:11434"
routing_tier = "4b"
timeout_seconds = 90
enabled = true
options = { num_ctx = 8192, temperature = 0.2, connect_timeout = 5 }

# LOCAL FALLBACK: Qwen3 4b Instruct - instruction-tuned for structured output!
[[enrichment.chain]]
name = "qwen3 4b instruct"
chain = "code_enrichment_models"
provider = "ollama"
model = "qwen3:4b-instruct"
url = "http://localhost:11434"
routing_tier = "4b"
timeout_seconds = 90
enabled = true
options = { num_ctx = 8192, temperature = 0.2 }


# NEW: Qwen3 8b - fallback if 4b struggles
[[enrichment.chain]]
name = "qwen3 8b"
chain = "code_enrichment_models"
provider = "ollama"
model = "qwen3:8b"
url = "http://localhost:11434"
routing_tier = "8b"
timeout_seconds = 120
enabled = true
options = { num_ctx = 8192, temperature = 0.2 }

# Final fallback - 14B Qwen2.5 for hard cases (known good)
[[enrichment.chain]]
name = "qwen2.5 14b instruct"
chain = "code_enrichment_models"
provider = "ollama"
model = "qwen2.5:14b-instruct-q4_K_M"
url = "http://localhost:11434"
routing_tier = "14b"
timeout_seconds = 180
enabled = true
options = { num_ctx = 12288, temperature = 0.2 }

# [[enrichment.chain]]
# name = "groq-70b"
# chain = "code_enrichment_models"
# provider = "gateway"
# model = "groq/llama3-70b-8192"
# routing_tier = "14b"
# enabled = true

# [[enrichment.chain]]
# name = "google-flash"
# chain = "code_enrichment_models"
# provider = "gemini"
# model = "gemini-1.5-flash"
# routing_tier = "nano"
# enabled = true

# ==============================================================================
# DOCUMENT Chain - For documentation enrichment
# ==============================================================================
# 
# 


# OLD: Qwen2.5 7b - replaced by Qwen3
# [[enrichment.chain]]
# name = "Qwen 2.5 7b Instruct"
# chain = "document_routes"
# provider = "ollama"
# model = "qwen2.5:7b-instruct"
# url = "http://localhost:11434"
# routing_tier = "7b"
# timeout_seconds = 120
# enabled = true
# options = { num_ctx = 8192, temperature = 0.2 }

# DESKTOP-NBHNN60 (Tailscale) - PRIMARY for docs
[[enrichment.chain]]
name = "desktop qwen3 4b docs"
chain = "document_routes"
provider = "ollama"
model = "qwen3:4b-instruct"
url = "http://100.64.0.6:11434"
routing_tier = "4b"
timeout_seconds = 90
enabled = true
options = { num_ctx = 8192, temperature = 0.2, connect_timeout = 5 }

# LOCAL FALLBACK: Qwen3 4b Instruct for docs (instruction-tuned)
[[enrichment.chain]]
name = "qwen3 4b instruct"
chain = "document_routes"
provider = "ollama"
model = "qwen3:4b-instruct"
url = "http://localhost:11434"
routing_tier = "4b"
timeout_seconds = 90
enabled = true
options = { num_ctx = 8192, temperature = 0.2 }


# NEW: Qwen3 8b fallback
[[enrichment.chain]]
name = "qwen3 8b"
chain = "document_routes"
provider = "ollama"
model = "qwen3:8b"
url = "http://localhost:11434"
routing_tier = "8b"
timeout_seconds = 120
enabled = true
options = { num_ctx = 8192, temperature = 0.2 }

# Final fallback: Local Qwen2.5 14B (known good)
[[enrichment.chain]]
name = "qwen2.5 14b Instruct"
chain = "document_routes"
provider = "ollama"
model = "qwen2.5:14b-instruct-q4_K_M"
url = "http://localhost:11434"
routing_tier = "14b"
timeout_seconds = 180
enabled = true
options = { num_ctx = 12288, temperature = 0.2 }

[[enrichment.chain]]
name = "MiniMax M2"
chain = "document_routes"
provider = "minimax"
model = "MiniMax-M2"
url = "https://api.minimax.io/anthropic/v1"  # Anthropic-compatible endpoint (needs /v1 for /messages path)
routing_tier = "7b"
timeout_seconds = 60
enabled = true

# ==============================================================================
# DEEPSEEK - Cheap cloud fallback for enrichment ($0.14/M input, $0.28/M output)
# ==============================================================================
# DeepSeek API is OpenAI-compatible. Great for batch enrichment when local
# models are busy or unavailable. Requires DEEPSEEK_API_KEY env var.

[[enrichment.chain]]
name = "deepseek-chat"
chain = "code_enrichment_models"
provider = "openai"  # Uses OpenAI-compatible adapter
model = "deepseek-chat"
url = "https://api.deepseek.com/v1"
routing_tier = "70b"  # High tier - only used when local models fail
timeout_seconds = 60
enabled = true
api_key_env = "DEEPSEEK_API_KEY"

[[enrichment.chain]]
name = "deepseek-chat-docs"
chain = "document_routes"
provider = "openai"  # Uses OpenAI-compatible adapter
model = "deepseek-chat"
url = "https://api.deepseek.com/v1"
routing_tier = "70b"  # High tier - only used when local models fail
timeout_seconds = 60
enabled = true
api_key_env = "DEEPSEEK_API_KEY"


# ==============================================================================
# INDEXING
# ==============================================================================
# 
# 
# 


[indexing]
exclude_dirs = [
    ".git",
    ".rag",
    "node_modules",
    "dist",
    "build",
    "__pycache__",
    ".venv",
    "venv",
    ".next",
    ".cache",
    ".pytest_cache",
    "coverage",
    ".DS_Store",
    "Thumbs.db",
    "tools/rag/quality_check",
]


# ==============================================================================
# LOGGING
# ==============================================================================
# 
# 
# 


[logging]
max_file_size_mb = 10
keep_jsonl_lines = 1000
enable_rotation = true
log_directory = "logs"
auto_rotation_interval = 0


# ==============================================================================
# Tool Envelope (TE) - Shell middleware for intelligent output handling
# ==============================================================================
# TE intercepts standard commands and returns enriched, ranked,
# progressively-disclosed output. The response stream IS the teaching.
# Philosophy: LLMs are alien intelligences that already know Unix.

[tool_envelope]
enabled = true  # Disabled: causing friction with duplicate searches + grep
passthrough_timeout_seconds = 30

[tool_envelope.workspace]
root = "/home/vmlinux/src/llmc"
respect_gitignore = true
allow_outside_root = false

[tool_envelope.telemetry]
enabled = true
path = ".llmc/te_telemetry.db"
capture_output = true  # Store actual command output for review (privacy/storage tradeoff)
output_max_bytes = 8192  # Max bytes to capture per command

[tool_envelope.grep]
enabled = true  # Disable enrichment, passthrough with logging only
preview_matches = 10
max_output_chars = 20000
timeout_ms = 5000

[tool_envelope.cat]
enabled = true  # Disable enrichment, passthrough with logging only
preview_lines = 50
max_output_chars = 30000

[tool_envelope.agent_budgets]
gemini-shell = 900000
claude-dc = 180000
qwen-local = 28000
human-cli = 50000
default = 16000


# ==============================================================================
# MCP CONFIGURATION - Model Context Protocol Server
# ==============================================================================
# stdio transport for Claude Desktop; HTTP deferred to later milestone.
# Config precedence: CLI → ENV → TOML → defaults

[mcp]
enabled = true
config_version = "v0"

# ==============================================================================
# CODE EXECUTION MODE - Anthropic "Code Mode" pattern (Phase 2)
# ==============================================================================
# When enabled, replaces 23 tools with 3 bootstrap tools + code execution.
# Claude navigates .llmc/stubs/ filesystem, reads tool definitions on-demand,
# writes Python code that imports and calls them. 98% token reduction.
# Reference: https://www.anthropic.com/engineering/code-execution-with-mcp

[mcp.code_execution]
enabled = false  # Disabled: was blocking write tools. Use classic mode for full capability.
stubs_dir = ".llmc/stubs"  # Generated tool stubs for Claude to browse
sandbox = "subprocess"  # "subprocess" | "docker" | "nsjail" (future)
timeout = 30
max_output_bytes = 65536
# Bootstrap tools exposed when code_execution.enabled = true
# All other tools become importable stubs, not MCP-registered tools
bootstrap_tools = ["list_dir", "read_file", "execute_code"]
# HYBRID OPTION: Enable write tools without full classic mode (best of both worlds)
# bootstrap_tools = ["list_dir", "read_file", "execute_code", "linux_fs_write", "linux_fs_edit", "run_cmd"]

[mcp.server]
# stdio transport - no host/port needed for Claude Desktop
transport = "stdio"
log_level = "info"

[mcp.auth]
# stdio doesn't need auth (process isolation)
mode = "none"

[mcp.tools]
# Filesystem access roots - all repos under src
allowed_roots = ["/home/vmlinux/src"]
enable_run_cmd = true  # M3: Command execution enabled

# Blacklist is just asking nicely - not real security.
# Real security: Docker (untrusted) vs hybrid mode (trusted).
# If you give an LLM bash, they can do anything anyway.
# run_cmd_blacklist = []  # Empty by default

# Custom executable tools (exposed as named tools)
# executables = { my_script = "scripts/my_script.sh", deploy = "scripts/deploy.sh" }

read_timeout = 10
exec_timeout = 30

[mcp.rag]
# RAG adapter settings
jit_context_enabled = true
default_scope = "repo"
top_k = 3
token_budget = 600

[mcp.limits]
max_request_bytes = 262144
max_response_bytes = 1048576

[mcp.observability]
# M4: Observability - metrics, logging, token audit
enabled = true
log_format = "json"  # "json" | "text"
log_level = "info"
include_correlation_id = true
# Metrics (in-memory for stdio; get_metrics tool to inspect)
metrics_enabled = true
# Token audit CSV trail
csv_token_audit_enabled = true
csv_path = "./artifacts/mcp_token_audit.csv"
retention_days = 0  # 0 = keep forever

# ==============================================================================
# RAG
# ==============================================================================
# RAG (Retrieval-Augmented Generation) settings

# ==============================================================================
# RAG
# ==============================================================================
# RAG (Retrieval-Augmented Generation) settings

[rag]
enabled = true

[rag.graph]
enable_expansion = true
neighbor_score_factor = 0.4
top_n_expand = 5
max_neighbors_per = 3
hub_penalty_threshold = 50

[scoring]
# P0 FIX: These values must be large enough to overcome semantic similarity
# bias toward keyword-rich docs. Previous values (0.08, -0.06) were too weak.
code_boost = 0.15        # Was 0.08 - boost code files in search
doc_penalty = -0.12      # Was -0.06 - penalize docs when looking for code
test_penalty = -0.10     # Was -0.08 - penalize test files
exact_match_boost = 0.35   # Huge boost for exact filename match
stem_match_boost = 0.30    # Big boost for stem match (router -> router.py)
partial_match_boost = 0.08
enable_intent_detection = true
# code_extensions and doc_extensions use defaults if omitted

[scoring.fusion]
method = "rrf"  # Options: "max", "rrf"
rrf_k = 60      # RRF smoothing constant

# ==============================================================================
# Profiles
# ==============================================================================
# Configuration profiles for different modes

[profiles.daily]
provider = "anthropic"
model = "claude-3-5-sonnet-latest"
temperature = 0.3

[profiles.daily.rag]
enabled = true

[profiles.yolo]
provider = "minimax"
model = "m2-lite"
temperature = 0.2

[profiles.yolo.rag]
enabled = true

# ==============================================================================
# BOXXIE - Qwen3-Next-80B-A3B on Athena (Strix Halo)
# ==============================================================================
# Your 80B MoE monster running at 32 t/s via Vulkan unified memory.
# 512 experts, 10 active per token = 3B effective compute per forward pass.
# Keep-alive: 1 hour. First request loads ~85GB, subsequent requests instant.

[profiles.boxxie]
provider = "ollama"
model = "qwen3-next-80b-nothink"
url = "http://athena:11434"
temperature = 0.6
timeout_seconds = 300  # Allow time for initial model load

[profiles.boxxie.rag]
enabled = true

[profiles.boxxie-tools]
provider = "ollama"
model = "qwen3-next-80b-tools"
url = "http://athena:11434"
temperature = 0.6
timeout_seconds = 300

# UTP (Unified Tool Protocol) Configuration
# Controls how tool calls are parsed from model responses.
# Boxxie uses XML tool calls in content, not OpenAI's tool_calls field.
[profiles.boxxie-tools.tools]
definition_format = "openai"  # How to send tool definitions to the model
call_parser = "auto"          # "auto" | "openai" | "anthropic" - auto detects XML or native
result_format = "openai"      # How to format tool results back to model

# ==============================================================================
# GPT-OSS-120B - via llama.cpp server (Vulkan)
# ==============================================================================
# 116B parameter model running on Athena via llama-server.
# Uses OpenAI-compatible API on port 8080.
# Supports native tool calling with excellent reasoning.

[profiles.gpt-oss]
provider = "openai"
model = "gpt-oss-120b-F16.gguf"
url = "http://athena:8080/v1"
temperature = 0.6
timeout_seconds = 300

[profiles.gpt-oss.rag]
enabled = true

[profiles.gpt-oss.tools]
definition_format = "openai"
call_parser = "auto"
result_format = "openai"




[enrichment.path_weights]
# Weight 1-10: lower = higher priority, 10 = back of the line
# Patterns are globs, matched against repo-relative paths

# Defaults (ship with these)
"src/**"        = 1
"lib/**"        = 1
"app/**"        = 1
"core/**"       = 1
"pkg/**"        = 2
"internal/**"   = 2
"cmd/**"        = 2

# Test code - deprioritized but not banished
"**/tests/**"   = 6
"**/test/**"    = 6
"**/__tests__/**" = 6
"*_test.py"     = 6
"test_*.py"     = 6
"*.test.ts"     = 6
"*.spec.js"     = 6

# Docs and config - the fatties
"docs/**"       = 8
"*.md"          = 7
".github/**"    = 9
"examples/**"   = 7

# Vendor trash - absolute back of the line
"vendor/**"     = 10
"node_modules/**" = 10  # shouldn't even be scanned but just in case
"third_party/**" = 10


# ==============================================================================
# MEDICAL DOMAIN CONFIGURATION
# ==============================================================================

[repository]
domain = "medical"
medical_subtype = "clinical_note"
institution = "default"
default_domain = "medical"

[repository.path_overrides]
"notes/**"     = "medical"
"labs/**"      = "medical"
"radiology/**" = "medical"
"discharge/**" = "medical"

[medical.gated_formats]
hl7  = { enabled = false, parser = "hl7apy", z_segment_map = "config/hl7_z_segments.toml" }
ccda = { enabled = false, parser = "lxml", namespace = "urn:hl7-org:v3", narrative_policy = "prefer_discrete" }

[embeddings.profiles.medical]
provider = "ollama"
model = "bge-m3"
dimension = 1024
capabilities = ["medical", "clinical", "multilingual"]
local_only = true
ollama_host = "http://athena:11434"

[embeddings.routes.medical]
profile = "medical"
index = "emb_medical"
sharing = "per-repo"
index_name_suffix = "{institution}-{repo_id}"

[embeddings.profiles.medical_doc]
provider = "clinical-longformer"
model = "yikuan8/Clinical-Longformer"
dimension = 768  # Clinical-Longformer output dimension
max_seq_tokens = 4096

[embeddings.profiles.medical_doc.clinical_longformer]
config_path = "config/models/clinical_longformer.json"
max_seq_tokens = 4096

[embeddings.routes.medical_doc]
profile = "medical_doc"
index = "emb_medical_doc"
sharing = "per-repo"
index_name_suffix = "{institution}-{repo_id}"
# ==============================================================================
# ENRICHMENT PROMPT - Configurable prompt for LLM enrichment
# ==============================================================================
# Template uses {path}, {line_start}, {line_end}, {snippet} placeholders.
# Use {{ and }} to escape literal braces in JSON examples.

[enrichment.prompt]
# Qwen3 4B is verbose - use terse instructions
template = """Return ONLY ONE VALID JSON OBJECT in ENGLISH.
No markdown, no comments, no extra text.
BE TERSE. Minimum words. No filler.

Output format (keys are FIXED):
{{"summary_120w":"...","inputs":["..."],"outputs":["..."],
"side_effects":["..."],"pitfalls":["..."],
"usage_snippet":"...","evidence":[{{"field":"summary_120w","lines":[{line_start},{line_end}]}}]}}

Rules:
- summary_120w: <=60 words. TERSE. State WHAT not HOW.
- inputs/outputs/side_effects/pitfalls: 1-3 word phrases MAX. [] if none.
- usage_snippet: 1-3 lines max, or "" if unclear.
- evidence: [{{"field":"summary_120w","lines":[{line_start},{line_end}]}}]
- No extra keys. Double quotes. No trailing commas.

Code:
{path} L{line_start}-{line_end}:
{snippet}

JSON:"""