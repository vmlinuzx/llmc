# Chaos Demon Resilience Test Report - 2026-01-15

This report details the findings of a resilience test focused on identifying potential failure points within the `llmc` codebase. The analysis prioritizes recently modified files to assess the impact of recent changes on system stability.

## Summary of Findings

| Check                       | Status | Findings                                                               | Severity |
| --------------------------- | ------ | ---------------------------------------------------------------------- | -------- |
| **Missing Config Handling** | <span style="color:orange">WARN</span> | `KeyError` risk in `pool_config.py`                                     | P1       |
| **Large Query Handling**    | <span style="color:green">OK</span>   | No client-side limits, relies on server handling                       | P2       |
| **Empty Database Handling** | <span style="color:red">FAIL</span> | Unhandled exceptions in `db_fts.py` and `graph_db.py` can cause crash  | P1, P2   |
| **Unicode Chaos**           | <span style="color:green">OK</span>   | Standard UTF-8 handling appears safe                                   | -        |
| **Timeout Behavior**        | <span style="color:green">OK</span>   | Network clients use explicit timeouts                                  | -        |

---

## Final Summary

The resilience of the `llmc` codebase is mixed. The system shows maturity in some areas, particularly in its handling of network timeouts and Unicode, but exhibits significant brittleness in file-based data ingestion and configuration parsing.

**Key Strengths:**
- **Network Resilience:** The use of `httpx` with explicit, sensible timeouts for both long-running queries and quick health checks is a major strength, preventing hangs from unresponsive services.
- **Robust Agent Configuration:** The `llmc_agent` configuration loader is well-designed, using defaults and graceful fallbacks to avoid crashes from missing configuration files or keys.
- **Graceful DB Queries:** Many database access patterns (e.g., in `enrichment_db_helpers.py` and `graph_db.py`) correctly handle empty results, returning empty lists or `None` instead of crashing.

**Critical Weaknesses:**
- **Brittle Data Ingestion:** The most severe issues are found in functions that read data from the filesystem. `fts_search` and `build_from_json` do not handle `FileNotFoundError` or other exceptions, leading to hard crashes if a database or JSON file is missing or malformed. This is a P1-level risk for the features that depend on them.
- **Inconsistent Config Handling:** While the agent's config is robust, other parts of the application are not. The parsing of worker pool configuration in `pool_config.py` will crash with a `KeyError` if required keys are missing, making the RAG enrichment service fragile.

**Recommendations:**
The highest priority should be to harden all file and database inputs. Every function that reads a file or database should be wrapped in a `try...except` block to handle I/O errors, parsing errors, or missing data. Instead of crashing, these functions should log the error and return a sensible default (e.g., an empty list or object), allowing the application to run in a degraded but stable state. Adopting the resilient patterns seen in `llmc_agent/config.py` across the entire codebase would significantly improve overall system stability.


---

## Detailed Analysis

### 1. Missing Config Handling

*Files Analyzed:*
* `llmc/main.py`
* `llmc_agent/config.py`
* `llmc/rag/pool_config.py`
* `llmc.toml`

**Observations:**

The project employs two different configuration loading strategies.
1.  **`llmc_agent/config.py`**: This configuration loader is highly resilient. It uses dataclasses with default values, merges multiple configuration sources, and handles missing files or keys gracefully by falling back to defaults. This is a robust implementation.
2.  **`llmc/core.py` (`load_config`)**: This function returns an empty dictionary (`{}`) if `llmc.toml` is not found. While this prevents a crash at load time, any downstream code that consumes this dictionary and expects specific keys will be vulnerable to `KeyError` exceptions if the file is missing.
3.  **`llmc/rag/pool_config.py`**: This module consumes the dictionary loaded by `llmc/core.py` and demonstrates the vulnerability mentioned above.

**Issues Found:**

*   **P1: Potential `KeyError` Crash in Worker Configuration**
    *   **File:** `llmc/rag/pool_config.py`
    *   **Function:** `load_pool_config`
    *   **Description:** The function directly accesses `w_data["id"]`, `w_data["type"]`, and `w_data["host"]` when parsing worker configurations from `llmc.toml`. If a worker is defined in the TOML file but is missing one of these required keys, the application will crash with a `KeyError`.
    *   **Recommendation:** Wrap the worker parsing logic in a `try...except KeyError` block. Log a detailed error message for the malformed worker entry and skip it, allowing the application to continue running with the correctly configured workers. This prevents a single misconfiguration from causing a total service failure.

---

### 2. Large Query Handling

*Files Analyzed:*
* `llmc_agent/backends/openai_compat.py`
* `llmc_agent/backends/ollama.py`
* `llmc/rag/cli.py`
* `llmc/main.py`

**Observations:**

The system does not implement any client-side limits on the size of queries or prompts that can be sent to the backend LLM services.
*   The `llmc/rag/cli.py` and `llmc/main.py` files accept user input of any length and pass it to the backend modules.
*   The backend modules (`openai_compat.py`, `ollama.py`) will attempt to send the entire payload to the respective API endpoints.

This design relies entirely on the server to handle large requests gracefully (e.g., by returning a `413 Payload Too Large` error). While the use of `response.raise_for_status()` in the backend clients is good practice and will report these server errors, the lack of a client-side "early exit" for obviously oversized-queries presents a minor resilience gap.

**Issues Found:**

*   **P2: Lack of Client-Side Input Size Validation**
    *   **Files:** `llmc/rag/cli.py`, `llmc/main.py`
    *   **Description:** User-provided queries can be of arbitrary length. A very large input could lead to significant (and potentially wasteful) data transfer before being rejected by the server. In a worst-case scenario, it could strain client-side memory or hit library-level limitations, though `httpx` is generally robust.
    *   **Recommendation:** Consider adding a pre-check on input size at the CLI level. If the input exceeds a reasonable threshold (e.g., a fraction of the model's context window), warn the user and offer to truncate or cancel the request. This provides a better user experience and adds a layer of defense.

---

### 3. Empty Database Handling

*Files Analyzed:*
* `llmc/rag/enrichment_db_helpers.py`
* `llmc/rag/db_fts.py`
* `llmc/rag/graph_db.py`

**Observations:**

The codebase shows a mix of highly resilient and highly brittle database handling patterns.
*   **Good:** `enrichment_db_helpers.py` and the main `graph_db.py` class are excellent examples of resilient code. They consistently check for missing files, handle empty query results, and wrap database calls in `try...except` blocks, returning empty collections (`[]` or `{}`) instead of crashing.
*   **Bad:** `db_fts.py` and the `build_from_json` function in `graph_db.py` are brittle. They raise exceptions on missing files or malformed data and do not handle them, leading to process termination.

**Issues Found:**

*   **P1: Unhandled Exceptions on Missing/Malformed DB in FTS**
    *   **File:** `llmc/rag/db_fts.py`
    *   **Function:** `fts_search`
    *   **Description:** The function does not handle exceptions raised by its internal helper functions (`_open_db`, `_detect_fts_table`, `_column_map`). If the RAG database doesn't exist, is empty, or doesn't conform to the expected schema, the function will crash instead of returning an empty search result. This makes the FTS search feature fragile.
    *   **Recommendation:** The `fts_search` function should be wrapped in a `try...except (RagDbNotFound, RuntimeError)` block. In the `except` block, it should log the error and return an empty list (`[]`). This would align its behavior with the more resilient parts of the codebase and prevent the entire command from failing.

*   **P2: Unhandled `FileNotFoundError` When Building Graph from JSON**
    *   **File:** `llmc/rag/graph_db.py`
    *   **Function:** `build_from_json`
    *   **Description:** This function, which builds the SQLite graph from a JSON file, raises an unhandled `FileNotFoundError` if the source `rag_graph.json` file does not exist. This will crash the building process.
    *   **Recommendation:** Wrap the file loading logic in a `try...except FileNotFoundError` block. The function should either log the error and return an empty `GraphDatabase` instance or propagate a more specific, handled exception.

---

### 4. Unicode Chaos

*Files Analyzed:*
* `llmc_agent/backends/openai_compat.py`
* `llmc_agent/backends/ollama.py`

**Observations:**

The backend clients use `httpx` to send JSON payloads. `httpx` defaults to UTF-8 encoding, which is the standard for web APIs and correctly handles Unicode characters, including emojis and other special characters. The system appears to be safe from crashes related to Unicode input, assuming the remote API endpoints are also compliant with UTF-8. No issues were found.

**Issues Found:**

*   None.

---

### 5. Timeout Behavior

*Files Analyzed:*
* `llmc_agent/backends/openai_compat.py`
* `llmc_agent/backends/ollama.py`
* `llmc/rag/work_queue.py`
* `llmc/rag/workers.py`

**Observations:**

The backend clients (`openai_compat.py`, `ollama.py`) make excellent use of explicit timeouts in their `httpx` network calls.
*   A long timeout (120-300 seconds) is used for the main generation requests, which is appropriate for potentially slow LLM responses.
*   A much shorter timeout (5 seconds) is correctly used for auxiliary calls like health checks and listing models.

This prevents the application from hanging indefinitely due to unresponsive network services.

**Issues Found:**

*   None.