sssss............FFFF...FFFsssssss.........sFssssssssssssssssssssssss.ss [ 79%]
sssss...........Fss                                                      [100%]
=================================== FAILURES ===================================
___________________ TestExecuteCode.test_call_tool_injection ___________________

self = <test_code_exec.TestExecuteCode object at 0x7a20cee791c0>

        def test_call_tool_injection(self):
            """Verify _call_tool is available in executed code namespace."""
            mock_caller = make_mock_tool_caller(
                {"test_tool": {"data": "mock_result", "meta": {}}}
            )
    
            result = execute_code(
                code="""
    result = _call_tool("test_tool", {"arg": "value"})
    print(f"Got: {result}")
    """,
                tool_caller=mock_caller,
            )
>           assert result.success
E           AssertionError: assert False
E            +  where False = CodeExecResult(success=False, stdout='', stderr='', return_value=None, error='RuntimeError: time.sleep blocked by pytest_ruthless. Use --allow-sleep or @pytest.mark.allow_sleep').success

/home/vmlinux/src/llmc/tests/mcp/test_code_exec.py:56: AssertionError
_____________ TestExecuteCode.test_import_stub_calls_injected_tool _____________

self = <test_code_exec.TestExecuteCode object at 0x7a20cee79880>
tmp_path = PosixPath('/tmp/pytest-of-vmlinux/pytest-47/test_import_stub_calls_injecte0')

        def test_import_stub_calls_injected_tool(self, tmp_path):
            """
            Critical test: Verify that imported stubs use builtins._call_tool.
    
            This was the bug - stubs imported _call_tool from the module which
            raised NotImplementedError, instead of using the injected version.
            """
            # Generate a test stub
            test_tool = Tool(
                name="my_test_tool",
                description="A test tool",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "query": {"type": "string", "description": "Test query"}
                    },
                    "required": ["query"],
                },
            )
    
            stubs_dir = tmp_path / "stubs"
            generate_stubs([test_tool], Path("stubs"), tmp_path)
    
            # Mock caller that records what was called
            calls = []
    
            def tracking_caller(name: str, args: dict) -> dict:
                calls.append((name, args))
                return {"data": "success", "meta": {}}
    
            # Execute code that imports and uses the stub
            result = execute_code(
                code="""
    from stubs import my_test_tool
    result = my_test_tool(query="test query")
    print(f"Result: {result}")
    """,
                tool_caller=tracking_caller,
                stubs_dir=stubs_dir,
            )
    
>           assert (
                result.success
            ), f"Execution failed: {result.error}\nstderr: {result.stderr}"
E           AssertionError: Execution failed: RuntimeError: time.sleep blocked by pytest_ruthless. Use --allow-sleep or @pytest.mark.allow_sleep
E             stderr: 
E           assert False
E            +  where False = CodeExecResult(success=False, stdout='', stderr='', return_value=None, error='RuntimeError: time.sleep blocked by pytest_ruthless. Use --allow-sleep or @pytest.mark.allow_sleep').success

/home/vmlinux/src/llmc/tests/mcp/test_code_exec.py:100: AssertionError
____________________ TestExecuteCode.test_builtins_cleanup _____________________

self = <test_code_exec.TestExecuteCode object at 0x7a20cee79ca0>

    def test_builtins_cleanup(self):
        """Verify builtins._call_tool is cleaned up after execution."""
        import builtins
    
        # Ensure clean state
        if hasattr(builtins, "_call_tool"):
            delattr(builtins, "_call_tool")
    
        result = execute_code(
            code='print("test")',
            tool_caller=lambda n, a: {},
        )
    
>       assert result.success
E       AssertionError: assert False
E        +  where False = CodeExecResult(success=False, stdout='', stderr='', return_value=None, error='RuntimeError: time.sleep blocked by pytest_ruthless. Use --allow-sleep or @pytest.mark.allow_sleep').success

/home/vmlinux/src/llmc/tests/mcp/test_code_exec.py:121: AssertionError
_____________________ TestExecuteCode.test_timeout_capture _____________________

self = <test_code_exec.TestExecuteCode object at 0x7a20cedc3920>

    def test_timeout_capture(self):
        """Test that stderr is captured on error."""
        result = execute_code(
            code='raise ValueError("test error")',
            tool_caller=lambda n, a: {},
        )
    
        assert not result.success
>       assert "ValueError" in result.error
E       AssertionError: assert 'ValueError' in 'RuntimeError: time.sleep blocked by pytest_ruthless. Use --allow-sleep or @pytest.mark.allow_sleep'
E        +  where 'RuntimeError: time.sleep blocked by pytest_ruthless. Use --allow-sleep or @pytest.mark.allow_sleep' = CodeExecResult(success=False, stdout='', stderr='', return_value=None, error='RuntimeError: time.sleep blocked by pytest_ruthless. Use --allow-sleep or @pytest.mark.allow_sleep').error

/home/vmlinux/src/llmc/tests/mcp/test_code_exec.py:135: AssertionError
__________________ test_run_untrusted_python_security_warning __________________

    def test_run_untrusted_python_security_warning():
        """Verify that the tool's docstring includes a prominent security warning."""
>       assert "WARNING" in code_exec.run_untrusted_python.__doc__
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AttributeError: module 'llmc_mcp.tools.code_exec' has no attribute 'run_untrusted_python'

/home/vmlinux/src/llmc/tests/mcp/test_code_exec_security.py:7: AttributeError
____________________ test_module_docstring_security_warning ____________________

    def test_module_docstring_security_warning():
        """Verify that the module docstring also contains a security warning."""
>       assert "SECURITY" in code_exec.__doc__
E       assert 'SECURITY' in '\nCode Execution Mode - Anthropic "Code Mode" pattern implementation.\n\nWhen enabled, replaces 23 MCP tools with 3 b...rts and calls them. 98% token reduction.\n\nReference: https://www.anthropic.com/engineering/code-execution-with-mcp\n'
E        +  where '\nCode Execution Mode - Anthropic "Code Mode" pattern implementation.\n\nWhen enabled, replaces 23 MCP tools with 3 b...rts and calls them. 98% token reduction.\n\nReference: https://www.anthropic.com/engineering/code-execution-with-mcp\n' = code_exec.__doc__

/home/vmlinux/src/llmc/tests/mcp/test_code_exec_security.py:13: AssertionError
_____________________________ test_tool_is_renamed _____________________________

    def test_tool_is_renamed():
        """Verify that run_untrusted_python exists and execute_code is an alias to it."""
>       assert hasattr(code_exec, "run_untrusted_python")
E       AssertionError: assert False
E        +  where False = hasattr(code_exec, 'run_untrusted_python')

/home/vmlinux/src/llmc/tests/mcp/test_code_exec_security.py:19: AssertionError
_______________________________ test_mcp_via_sse _______________________________
async def functions are not natively supported.
You need to install a suitable plugin for your async framework, for example:
  - anyio
  - pytest-asyncio
  - pytest-tornasync
  - pytest-trio
  - pytest-twisted
__________________ test_all_tool_parameters_have_descriptions __________________

    def test_all_tool_parameters_have_descriptions():
        """
        Validates that every parameter in every tool's inputSchema has a description.
        """
        missing_descriptions = []
    
        for tool in TOOLS:
            schema = tool.inputSchema
            if "properties" in schema:
                for param, definition in schema["properties"].items():
                    if "description" not in definition or not definition["description"]:
                        missing_descriptions.append(f"{tool.name}.{param}")
    
>       assert not missing_descriptions, (
            "The following tool parameters are missing a 'description' field:\n"
            + "\n".join(missing_descriptions)
        )
E       AssertionError: The following tool parameters are missing a 'description' field:
E         linux_fs_write.mode
E         linux_fs_mkdir.exist_ok
E       assert not ['linux_fs_write.mode', 'linux_fs_mkdir.exist_ok']

/home/vmlinux/src/llmc/tests/mcp/test_tool_schemas.py:17: AssertionError
=========================== short test summary info ============================
SKIPPED [5] tests/mcp/manual_mcp_test.py: Standalone test script - run directly with python
SKIPPED [7] tests/mcp/test_fs.py: Standalone test script - run directly with python
SKIPPED [1] tests/mcp/test_mcp_http.py: Standalone test script - run directly with python
SKIPPED [20] tests/mcp/test_observability.py: Standalone test script - run directly with python
SKIPPED [4] tests/mcp/test_rag.py: Standalone test script - run directly with python
SKIPPED [7] tests/mcp/test_smoke.py: Standalone test script - run directly with python
SKIPPED [2] tests/mcp/test_tools_visibility_and_metrics.py:25: No accessible tool registry on server module.
FAILED tests/mcp/test_code_exec.py::TestExecuteCode::test_call_tool_injection
FAILED tests/mcp/test_code_exec.py::TestExecuteCode::test_import_stub_calls_injected_tool
FAILED tests/mcp/test_code_exec.py::TestExecuteCode::test_builtins_cleanup - ...
FAILED tests/mcp/test_code_exec.py::TestExecuteCode::test_timeout_capture - A...
FAILED tests/mcp/test_code_exec_security.py::test_run_untrusted_python_security_warning
FAILED tests/mcp/test_code_exec_security.py::test_module_docstring_security_warning
FAILED tests/mcp/test_code_exec_security.py::test_tool_is_renamed - Assertion...
FAILED tests/mcp/test_mcp_sse.py::test_mcp_via_sse - Failed: async def functi...
FAILED tests/mcp/test_tool_schemas.py::test_all_tool_parameters_have_descriptions
9 failed, 36 passed, 46 skipped in 4.76s
